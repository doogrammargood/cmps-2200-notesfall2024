{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45162/1251851185.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display,HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.prompt{width: 0px; min-width: 0px; visibility: collapse}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div#notebook {\n",
       " font-family: \"Exo_2\", sans-serif;\n",
       "}\n",
       "\n",
       ".rendered_html h1,\n",
       ".text_cell_render h1 {\n",
       " color: #126dce;\n",
       " font-size: 220%;\n",
       " text-align: center;\n",
       " font-weight: lighter;\n",
       "}\n",
       ".rendered_html h2,\n",
       ".text_cell_render h2 {\n",
       " text-align: center;\n",
       " font-size: 170%;\n",
       " color: #126dce;\n",
       " font-style: normal;\n",
       " font-weight: lighter;\n",
       "}\n",
       ".rendered_html h3,\n",
       ".text_cell_render h3 {\n",
       " font-size: 150%;\n",
       " color: #126dce;\n",
       " font-weight: lighter;\n",
       " text-decoration: italic;\n",
       " font-style: normal;\n",
       "}\n",
       ".rendered_html h4,\n",
       ".text_cell_render h4 {\n",
       " font-size: 120%;\n",
       " color: #126dce;\n",
       " font-weight: underline;\n",
       " font-style: normal;\n",
       "}\n",
       ".rendered_html h5,\n",
       ".text_cell_render h5 {\n",
       " font-size: 100%;\n",
       " color: #2f2f2f;\n",
       " font-weight: lighter;\n",
       " text-decoration: underline;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup\n",
    "from IPython.core.display import display,HTML\n",
    "display(HTML('<style>.prompt{width: 0px; min-width: 0px; visibility: collapse}</style>'))\n",
    "display(HTML(open('../rise.css').read()))\n",
    "\n",
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5, rc={'figure.figsize':(12, 6)})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CMPS 2200\n",
    "# Introduction to Algorithms\n",
    "\n",
    "## Probability Theory for Randomized Algorithms\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Agenda:\n",
    "\n",
    "- Random variables and expectations\n",
    "- Making random choices in algorithms\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why make random decisions?\n",
    "\n",
    "Suppose you could flip a coin to make a decision in an algorithm, rather than relying on inputs. So:\n",
    "\n",
    "```python\n",
    "if (random() > 0.5):\n",
    "    print('heads')\n",
    "else:\n",
    "    print('tails')\n",
    "```\n",
    "\n",
    "Why would we ever do this?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Benefits\n",
    "\n",
    "Randomized algorithms can often be:\n",
    "\n",
    "1. easier to understand (?)\n",
    "2. faster (?)\n",
    "3. more robust to adversarial input (cryptography)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic Probability Definitions\n",
    "\n",
    "A probability space consists of a **sample space** $\\Omega$ representing the set of possible *outcomes*, and a **probability measure** $\\mathbf{P}: \\Omega \\rightarrow \\mathbb{R}_+$. The probability measure $\\mathbf{P}$ must satisfy three conditions:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- **Nonnegativity**: $\\mathbf{P}\\left[{A}\\right] \\in [0,1]$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- **Additivity**: For any two disjoint events $A$ and $B$ (i.e., $A \\cap B = \\emptyset$): $\\mathbf{P}\\left[{A \\cup B}\\right] =  \n",
    "  \\mathbf{P}\\left[{A}\\right] + \\mathbf{P}\\left[{B}\\right]$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- **Normalization**: $\\mathbf{P}\\left[{\\Omega}\\right] = 1$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Let $\\Omega$ be the set of all outcomes of a pair of 6-sided dice.\n",
    "\n",
    "$\\Omega = \\{(1,1), (1,2),...,(2,1),...,(6,6)\\}$\n",
    "\n",
    "How many outcomes are there if the dice sum to 3? To 4?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "The dice sum to 3 if we roll any of $\\{(1, 2), (2, 1)\\}$\n",
    "\n",
    "They sum to 4 if we roll any of $\\{(1, 3), (2, 2), (3, 1)\\}$.\n",
    "\n",
    "What is the probability of rolling a 3? ... a 4?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Every outcome is disjoint and has probability 1/36. So the probability of rolling a 3 is 2/36 = 1/18 and the probability of rolling a 4 is 3/36 = 1/12.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "> If we want to consider multiple, possibly overlapping events. Then, the **union bound** is helpful:\n",
    ">\n",
    "> $$\\mathbf{P}\\left[{\\bigcup_{0 \\leq i < n} A_i}\\right] \\leq \\sum_{i=0}^{n-1} \\mathbf{P}\\left[{A_i}\\right]$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional Probabilities\n",
    "\n",
    "We may want to know the probability of an event $A$ with \"partial knowledge.\"  It is very common to want to know the **conditional probability** of an event $A$ *given* that $B$ occurs ($\\mathbf{P}\\left[{B}\\right] > 0$)? \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **conditional probability** of an event $A$ *given* that $B$ occurs ($\\mathbf{P}\\left[{B}\\right] > 0$) is:\n",
    "\n",
    "$$\\mathbf{P}\\left[{A \\mid B}\\right] = \\frac{\\mathbf{P}\\left[{A \\cap B}\\right]}{\\mathbf{P}\\left[{B}\\right]}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Example\n",
    "\n",
    "What is the probability that the first die comes up 1 given that the sum of a pair of dice is 4?\n",
    "\n",
    "$$\\mathbf{P}\\left[{A \\mid B}\\right] = \\frac{\\mathbf{P}\\left[{A \\cap B}\\right]}{\\mathbf{P}\\left[{B}\\right]}$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$A = \\{(1,1),(1,2),(1,3),(1,4),(1,5),(1,6)\\}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$B = \\{(1,3),(2,2),(3,1)\\}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "${A \\cap B} = \\{(1,3)\\}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$\\mathbf{P}\\left[{A \\mid B}\\right] = \\frac{|A \\cap B|}{|B|} = \\frac{1}{3}.$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Law of Total Probability\n",
    "\n",
    "A useful fact that relates conditional probabilities to non-conditional probabilities is the **Law of Total Probability**:\n",
    "\n",
    "Let $\\Omega$ be a sample space and let $A_0, \\ldots, A_{n-1}$ be a set of events that partition  $\\Omega$ such that $\\mathbf{P}\\left[{A_i}\\right] > 0$ for all $0 \\le i < n.$ Then, for any event $B$ we have that: \n",
    "    \n",
    "$\\begin{array}{lll}  \n",
    "\\mathbf{P}\\left[{B}\\right]  & = & \\displaystyle\\sum_{i=0}^{n-1} \\mathbf{P}\\left[{B \\cap A_i}\\right]  \n",
    "\\\\  \n",
    "& = &  \\sum_{i=0}^{n-1} \\mathbf{P}\\left[{A_i}\\right]\\mathbf{P}\\left[{B \\mid A_i}\\right]  \n",
    "\\end{array}$\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Intuition: the probability of an event $B$ is just the sum of its probability over all potential disjoint situations. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "Suppose (this example is fictional) there are two variations of the flu vaccine: $A_1$ and $A_2$. \n",
    "\n",
    "The probability that a person will receive $A_1$ is $0.4$ and the probability that a person will receive $A_2$ is $0.6$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "A person who receives vaccine $A_1$ has a $12\\%$ chance of catching the flu this season\n",
    "\n",
    "A person who received vaccine $A_2$ has a $5\\%$ chance of catching the flu.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "What is the probability $\\mathbf{P}\\left[B\\right]$ that a vaccinated person will catch the flu?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "What is the probability $\\mathbf{P}\\left[B\\right]$ that a vaccinated person will catch the flu?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- $\\mathbf{P}\\left[{A_1}\\right] = 0.4$\n",
    "- $\\mathbf{P}\\left[{A_2}\\right] = 0.6$\n",
    "- $\\mathbf{P}\\left[{B \\mid A_1}\\right] = 0.12$\n",
    "- $\\mathbf{P}\\left[{B \\mid A_2}\\right] = 0.05$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\mathbf{P}\\left[{B}\\right] &=& \\sum_{i=0}^{n-1} \\mathbf{P}\\left[{A_i}\\right]\\mathbf{P}\\left[{B \\mid A_i}\\right]  \\\\ \\\\\n",
    "&=& \\mathbf{P}\\left[{A_1}\\right]\\mathbf{P}\\left[{B \\mid A_1}\\right] + \\mathbf{P}\\left[{A_2}\\right]\\mathbf{P}\\left[{B \\mid A_2}\\right] \\\\ \\\\\n",
    "&=& 0.4 * 0.12 + 0.6 * 0.05 \\\\ \\\\\n",
    "&=& 0.078\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "For a second example of this, see Vol 1, Ch 18 of the book.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Independence\n",
    "\n",
    "Another useful concept is **independence**. We say that two events $A$ and $B$ are independent when $\\mathbf{P}\\left[{A \\cap B}\\right] = \\mathbf{P}\\left[{A}\\right] \\cdot \\mathbf{P}\\left[{B}\\right].$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Variables and Expectations\n",
    "\n",
    "A **random variable** $X$ is a real-valued function on the outcomes of an experiment, i.e. $X : \\Omega\\to \\mathbb{R}$. We will consider only discrete random variables, meaning that we assume $|\\Omega|<\\infty$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Even more simply, we will most often use **indicator random variables**, which map outcomes to 0 or 1. \n",
    "\n",
    "$$X(d_1,d_2) =  \n",
    "\\left\\{  \n",
    "\\begin{array}{ll}  \n",
    "1 \\text{    if}~d_1 = d_2  \n",
    "\\\\  \n",
    "0 \\text{    if}~d_1 \\not= d_2 ~.  \n",
    "\\end{array}  \n",
    "\\right.$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "Note: The term \"random variable\" is somewhat odd. Really it's just a function, but we use this terminology since its output is dependent on a random process, and it is useful to look at functions of outcomes (which have probabilities under a given probability measure). \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Expectation\n",
    "\n",
    "The **expectation** of a random variable $X$ defined for a probability space $(\\Omega, \\mathbf{P})$ is denoted:\n",
    "\n",
    "$$\\mathbf{E}_{\\Omega,\\mathbf{P}}[X] = \\sum_{y \\in \\Omega} X(y) \\cdot  \n",
    "\\mathbf{P}\\left[\\{y\\}\\right].$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "We usually write this more conveniently as:\n",
    "\n",
    "$$ \\mathbf{E}\\left[{X}\\right] = \\sum_{x} x \\cdot \\mathbf{P}\\left[X = x\\right]. $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "$$ \\mathbf{E}\\left[{X}\\right] = \\sum_{x} x \\cdot \\mathbf{P}\\left[X = x\\right]. $$\n",
    "\n",
    "Let $X$ be the value of a pair of dice. $X$ can take on values between 2 and 12, where each has a certain probability based on the possible ways the dice can sum to that value.  What is the expected value of the value of a pair of unbiased dice? It is:\n",
    "\n",
    "\n",
    "$$\n",
    "2\\cdot\\frac{1}{36} + 3\\cdot\\frac{2}{36} + 4\\cdot\\frac{3}{36} + \\cdots + 11\\cdot\\frac{2}{36} + 12\\cdot\\frac{1}{36}$$\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Intuitively, the expectation is really just a weighted average of the outcomes of the random variable.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "expectation = 0\n",
    "for d1 in range(1,7):\n",
    "    for d2 in range(1,7):\n",
    "        expectation += (d1+d2)/36\n",
    "print(expectation) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Let's look at coin flips where we have equal probability of heads/tails. Let $X$ be the number of flips until we get heads. What is $E[X]$?\n",
    "\n",
    "Observe that we have two cases:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- The first flip is heads. \n",
    "    - This happens with probability $1/2$. \n",
    "    - The number of flips is 1.\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The first flip is tails. \n",
    "    - This happens with probability $1/2$. \n",
    "    - The number of flips is 1 plus the expected number of flips to get a heads."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$E[X] = \\frac{1}{2}\\cdot 1 + \\frac{1}{2} (1 + E[X])$$\n",
    "\n",
    "\n",
    "Solving for $E[X]$, what do we get?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$E[X] = 2$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "> More generally, the expected number of trials to get an outcome of probability $p$ is $1/p$. (We'll see this later.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Useful Facts\n",
    "\n",
    "We will make repeated use of some basic facts about the expectations of random variables.\n",
    "\n",
    "First, we can easily treat the expected value of functions of random variables:\n",
    "    \n",
    "$$ \\mathbf{E}\\left[f(X)\\right] = \\sum_{x} f(x) \\cdot \\mathbf{P}\\left[X = x\\right]. $$\n",
    "\n",
    "We also have **linearity of expectations** for random variables:\n",
    "\n",
    "$$ \\mathbf{E}\\left[X + Y\\right] = \\mathbf{E}\\left[X\\right] + E\\left[ Y\\right]. $$\n",
    "\n",
    "This identity will help us simplify the calcuation of the *expected* work and span of an algorithm. Linearity of expectations holds even when the random variables $X$ and $Y$ are not independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Markov's Bound</h3>\n",
    "\n",
    "A useful bound is Markov's Bound:\n",
    "\n",
    "Let $X:\\Omega \\to \\mathbb{R}_+$ be a random variable that takes non-negative values. Then for any $a>0$,\n",
    "\n",
    "$\\mathbf{P}[X\\geq a]\\leq\\frac{\\mathbf{E}[X]}{a}$.\n",
    "\n",
    "The proof is simple: $a\\mathbf{P}[X\\geq a]\\leq \\sum_{x\\geq a} x \\mathbf{P}[X=x] \\leq \\mathbf{E}[X] $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chebyshev's inequality:\n",
    "\n",
    "Let $X$ be a random variable with mean $\\mu$ and variance $\\sigma^2$. For any $k>0$,\n",
    "\n",
    "$\\mathbf{P}(|X-\\mu| \\geq k\\sigma)\\leq \\frac{1}{k^2}$\n",
    "\n",
    "\"It's unlikely that $X$ is far from the mean if the standard deviation is small.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does all this relate to algorithms?\n",
    "\n",
    "- The sample space we must consider for an algorithm is the set of decisions made by the algorithm. \n",
    "    - We try to come up with random variables that map outcomes to a way to measure work/span.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- It is usually difficult to reason directly about the overall outcome. \n",
    "    - We try to identify when choices are independent or conditional upon one another.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- For correctness, some executions may produce the wrong output while others are correct. \n",
    "    - We usually seek to have a very high *probability of correctness*. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- For work/span, some executions may take longer than others. \n",
    "    - We seek to provide asymptotic bounds on the *expected* work or span of a randomized algorithm. \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Intuitively, this is the weighted average of the run times over all possible sets of random choices. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Ultimately, if we choose to make random choices in our algorithm, we must relax our notion of correctness and our definitions of work/span.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Example</h3>\n",
    "\n",
    "Given a polynomial like $P(x,y)=(x+y)(x-y)-x^2+y^2$, is it equal to 0?\n",
    "\n",
    "Randomized algorithm: Choose several points at random $(x_0,y_0),(x_1,y_1),(x_2,y_2)\\dots$ and evaluate $P(x_i,y_i)$.\n",
    "\n",
    "The obvious deterministic way to do this is by expanding out $P(x,y)$ and checking that all coefficients are $0$. But this could take a long time if there are many products."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Random Algorithms\n",
    "\n",
    "Usually, random algorithms are defined by the work and probability of correctness.\n",
    "\n",
    "<img src=\"figures/Randomised_Complexity_Classes_2.svg\" style=\"background-color:white;\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- PP (Probabilistic polynomial time). The descisions problems that can be solved in polynomial time with probability greater that 1/2. Not feasible in practice.\n",
    "\n",
    "- BQP (Bounded error quantum polynomial time). The decision problems that can be solved using quantum computation and polyonmially many quantum operations that give the correct answer at least 2/3 of the time.\n",
    "\n",
    "- BPP (Bounded error probabilistic polynomial time). Also called a **Monte Carlo** randomized algorithm. They run in polynomial time and give the correct answer at least 2/3 of the time.\n",
    "    - Example: [The Miller-Rabin Primality Test](https://en.wikipedia.org/wiki/Miller%E2%80%93Rabin_primality_test)\n",
    "\n",
    "- RP (Randomized polynomial time). Runs in polynomial time and gives the correct answer all the time when the answer is 'No'. Gives the correct answer with probability at least 1/2 when the answer is 'Yes'.\n",
    "\n",
    "- Co-RP Like RP, but switch \"yes\" and \"no\".\n",
    "    - Example check if a polynomial $P(x,y)$ is $0$ everywhere.\n",
    "\n",
    "- ZPP (Zero error probabilistic polynomial time.) Also called a **Las Vegas** randomized algorithm. Runs in polynomial time and always gives the correct answer.\n",
    "    - Example: [Quicksort](https://cmps-2200.github.io/cmps-2200-notes/module-05-random/03-quicksort/01-quicksort.slides.html#/) \n",
    "\n",
    "Open question: Are these complexity classes actually different?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coming Up\n",
    "\n",
    "We will look at the probabilistic method and bloom filters."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "autolaunch": true,
   "controls": false,
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "simple",
   "transition": "fade"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
