{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Further models of computation</h1>\n",
    "\n",
    "<h2>Last time</h2>\n",
    "\n",
    "We discussed the Turing Machine, which is the ultimate model of computation. We explained the Church-Turing Thesis, which asserts that Turing Machines are powerful enough to describe any type of algorithm that can be devised in this universe.\n",
    "\n",
    "<h2>This time</h2>\n",
    "We will explore other computational models. The <i>Extended Church-Turing Thesis</i> asserts that Turing Machines can simulate all models of computation with only polynomial overhead.\n",
    "\n",
    "<h2>Random Access Machines</h2>\n",
    "The computers that we use today are different from Turing Machines. One difference is that Turing Machines are ideal mathematical objects, with infinite memory that never err. Another important difference is that computers have <i>random access memory</i>, meaning that memory can be address, and we can look up the value stored at a single address in a single step.\n",
    "\n",
    "The architecture of a random access machine can be very complicated. A simplified (but still complex) architecture called MIX is often used for pedagogical reasons. It is based on the original computer architecture described by John Von Neumann in the 50's. [Historical resource](https://web.mit.edu/STS.035/www/PDFs/edvac.pdf).\n",
    "\n",
    "The random access machine consists of\n",
    "\n",
    "1. A central processing unit (CPU)\n",
    "    - The CPU has a small number of <i>registers</i> that can contain information.\n",
    "    - It can also perform a finite number of predefined <i>operations</i>, like addition, multiplication and comparison, usually implemented as circuits.\n",
    "    - It can perform <i>load</i> and <i>jump</i> operations to interact with main memory.\n",
    "2. Main memory, a indexed set of memory addresses. We will assume this is infinite.\n",
    "\n",
    "For each operation that the CPU can perform, we must ask about the cost. We usually assume for simplicity that each operation takes $1$ unit of time. However, this is a simplification: It takes longer to add numbers with more digits. A more detailed treatment can be found in The Art of Computer Programmming.\n",
    "\n",
    "<h3>Turing Machines Vs Random Access</h3>\n",
    "\n",
    "Turing Machines and Random Access Machines can simulate each other with polynomial overhead. This means that if we talk about an algorithm running \"in linear time\" we need to specify which type of machine it runs on. It may run in linear time on a RAM machine, but not on a standard Turing Machine.\n",
    "\n",
    "In this class, our default model of computation is the RAM machine. Each time we encounter a problem, we should agree on the costs of the operations that we are allowed to use in an algorithm to solve the problem. Usually, we assume that all operations take $1$ unit of time.\n",
    "\n",
    "<h3>Other models of computation</h3>\n",
    "There are many of models of computation:\n",
    "\n",
    "1. Circuit models\n",
    "    - Different models depending on different allowed gates.\n",
    "2. Quantum models:\n",
    "    - Quantum Turing machine\n",
    "    - quantum circuit model\n",
    "    - quantum annealing model\n",
    "    - measurement-based quantum computation\n",
    "3. Analog computation (but how to account for errors?)\n",
    "    - DNA computing\n",
    "    - [Astrolabe](https://en.wikipedia.org/wiki/Astrolabe)\n",
    "    - Soap bubble computer\n",
    "4. [Parallel computing models](https://snir.cs.illinois.edu/listed/C27.pdf) [More rigorous source](https://snir.cs.illinois.edu/listed/C27.pdf)\n",
    "    - Fixed number of processors, or does the number grow with the problem size?\n",
    "    - How do the processors communicate? PRAM?.\n",
    "5. Random computing models\n",
    "    - Is the answer allowed to be random (Monte Carlo)?\n",
    "    - Or is just the runtime allowed to be random (Las Vegas)?\n",
    "    - How often does the computer need to give the correct answer?\n",
    "\n",
    "All known models of computation can be simulated by a standard Turing machine with polynomial overhead, except for the quantum models. It is believed that simulating quantum models on a standard Turing Machine require exponential overhead, but this has not been proven. The formal problem is $P\\stackrel{?}{=}BQP$, which is open and is unlikely to be solved anytime soon.\n",
    "\n",
    "For each model of computation, we can obtain complexity classes by limiting the resources of the model. For example, [Nick's Class](https://en.wikipedia.org/wiki/NC_(complexity)), $NC$ is the set of ploblems that can be solved in polylogarithmic time on a parallel computer, where the number of processors can grow polynomially in the problem's instance size. This is all hyper-specific, so you can see that the number of complexity classes will be massive. You can find a list of all known complexity classes and their relationships in the [complexity zoo](https://complexityzoo.net/Complexity_Zoo).\n",
    "\n",
    "<img src=\"complexity_venn_diagram.jpg\" width=60%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Why is polynomial considered \"efficient\"?</h3>\n",
    "\n",
    "We have said that algorithms that run in poylnomial time on a standard Turing Machine are considered to be \"efficient.\" One reason that we make this choice is due to the Extended Church-Turing Thesis, which implies that the algorithm runs in polynomial time on one mode of computation (except quantum computation), then it will also run in polynomial time on any other model of computation.\n",
    "\n",
    "Usually, whenever a problem has a polynomial-time solution, it has a small polynomial time solution, where the degree of the polynomial is at most, say, $5$. However, the [time-hierarchy theorem](https://en.wikipedia.org/wiki/Time_hierarchy_theorem) states that for every polynomial $P$, there is some problem whose best algorithm requires a runtime of $P$.\n",
    "\n",
    "Another reason to define efficiency in terms of \"polynomial time\" is that it abstracts away from the concrete runtime and instead focuses on larger, asymptotic behavior. Computers are always getting faster and more powerful according to Moore's Law. If we focus on the particular runtime of particular instances, our knowledge will be out of date in 18 months. Large scales, the exact coefficients of the polnomials don't matter. We are more interested in the behavior of the algorithm on large inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
